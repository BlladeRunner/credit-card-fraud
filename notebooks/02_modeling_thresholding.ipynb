{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27128e4",
   "metadata": {},
   "source": [
    "# Notebook 02 — Modeling & Threshold Tuning\n",
    "\n",
    "Goal:\n",
    "- Train a baseline fraud detection model\n",
    "- Evaluate using metrics suitable for extreme class imbalance\n",
    "- Demonstrate how decision threshold affects precision, recall and business trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import CFG\n",
    "from src.io import load_csv\n",
    "from src.split import stratified_split\n",
    "from src.models import train_logreg_baseline\n",
    "from src.evaluation import evaluate_at_threshold\n",
    "from src.thresholding import (\n",
    "    find_threshold_for_min_precision,\n",
    "    find_threshold_for_min_recall,\n",
    "    find_threshold_min_cost,\n",
    ")\n",
    "from src.viz import plot_pr_curve\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/raw/creditcard.csv\"\n",
    "df = load_csv(DATA_PATH)\n",
    "\n",
    "target_col = CFG.target_col\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = stratified_split(\n",
    "    df=df,\n",
    "    target_col=target_col,\n",
    "    test_size=CFG.test_size,\n",
    "    seed=CFG.seed,\n",
    ")\n",
    "\n",
    "train_df[target_col].mean(), test_df[target_col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[target_col])\n",
    "y_train = train_df[target_col].values\n",
    "\n",
    "X_test = test_df.drop(columns=[target_col])\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e398487",
   "metadata": {},
   "source": [
    "## Baseline model choice\n",
    "\n",
    "I start with Logistic Regression because:\n",
    "- it’s fast and strong as a baseline\n",
    "- it outputs probabilities (required for threshold tuning)\n",
    "- with `class_weight=\"balanced\"` it handles imbalance better than a naive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = train_logreg_baseline(X_train, y_train, seed=CFG.seed)\n",
    "y_prob = trained.predict_proba(X_test)\n",
    "\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b614b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(y_test, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_metrics = evaluate_at_threshold(\n",
    "    y_true=y_test,\n",
    "    y_prob=y_prob,\n",
    "    threshold=0.5,\n",
    "    cost_fn=CFG.cost_false_negative,\n",
    "    cost_fp=CFG.cost_false_positive,\n",
    ")\n",
    "default_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58383fa",
   "metadata": {},
   "source": [
    "## Why threshold tuning matters\n",
    "\n",
    "In fraud detection, threshold is not \"0.5 by default\".\n",
    "It directly controls the trade-off:\n",
    "- Higher threshold → fewer alerts → higher precision, lower recall (more missed fraud)\n",
    "- Lower threshold → more alerts → higher recall, lower precision (more false positives)\n",
    "\n",
    "Next, we choose thresholds based on constraints:\n",
    "1) Minimum Precision (control alert quality)\n",
    "2) Minimum Recall (avoid missing fraud)\n",
    "3) Minimum Expected Cost (FN vs FP cost trade-off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min_p = find_threshold_for_min_precision(y_test, y_prob, min_precision=CFG.min_precision)\n",
    "metrics_min_p = evaluate_at_threshold(\n",
    "    y_true=y_test,\n",
    "    y_prob=y_prob,\n",
    "    threshold=t_min_p,\n",
    "    cost_fn=CFG.cost_false_negative,\n",
    "    cost_fp=CFG.cost_false_positive,\n",
    ")\n",
    "t_min_p, metrics_min_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min_r = find_threshold_for_min_recall(y_test, y_prob, min_recall=CFG.min_recall)\n",
    "metrics_min_r = evaluate_at_threshold(\n",
    "    y_true=y_test,\n",
    "    y_prob=y_prob,\n",
    "    threshold=t_min_r,\n",
    "    cost_fn=CFG.cost_false_negative,\n",
    "    cost_fp=CFG.cost_false_positive,\n",
    ")\n",
    "t_min_r, metrics_min_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b69404",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min_cost = find_threshold_min_cost(\n",
    "    y_test, y_prob,\n",
    "    cost_fn=CFG.cost_false_negative,\n",
    "    cost_fp=CFG.cost_false_positive,\n",
    ")\n",
    "\n",
    "metrics_min_cost = evaluate_at_threshold(\n",
    "    y_true=y_test,\n",
    "    y_prob=y_prob,\n",
    "    threshold=t_min_cost,\n",
    "    cost_fn=CFG.cost_false_negative,\n",
    "    cost_fp=CFG.cost_false_positive,\n",
    ")\n",
    "t_min_cost, metrics_min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.DataFrame([\n",
    "    {\"strategy\": \"default_0.5\", **default_metrics},\n",
    "    {\"strategy\": f\"min_precision_{CFG.min_precision}\", **metrics_min_p},\n",
    "    {\"strategy\": f\"min_recall_{CFG.min_recall}\", **metrics_min_r},\n",
    "    {\"strategy\": f\"min_cost_fn{CFG.cost_false_negative}_fp{CFG.cost_false_positive}\", **metrics_min_cost},\n",
    "])\n",
    "\n",
    "# make confusion matrix readable\n",
    "compare[\"TN_FP_FN_TP\"] = compare[\"confusion_matrix\"].apply(lambda x: [x[0][0], x[0][1], x[1][0], x[1][1]])\n",
    "compare.drop(columns=[\"confusion_matrix\"], inplace=True)\n",
    "\n",
    "compare[[\"strategy\", \"threshold\", \"pr_auc\", \"precision\", \"recall\", \"f1\", \"expected_cost\", \"TN_FP_FN_TP\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887cd0d",
   "metadata": {},
   "source": [
    "## Conclusion (Notebook 02)\n",
    "\n",
    "- Baseline Logistic Regression provides a meaningful starting point.\n",
    "- PR-AUC is used as the core metric due to extreme imbalance.\n",
    "- Default threshold (0.5) is not optimal.\n",
    "- Threshold tuning changes the trade-off between false positives and missed fraud.\n",
    "- The final threshold selection depends on business constraints:\n",
    "  - high precision → reduce alert fatigue\n",
    "  - high recall → reduce missed fraud\n",
    "  - cost-based → optimize operational + fraud loss trade-off\n",
    "\n",
    "Next notebook: deep dive into false positives/false negatives and business impact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
